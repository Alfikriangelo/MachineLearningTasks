{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQwQ7Vw09wxG79mjDdiU5G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alfikriangelo/MachineLearningTasks/blob/main/10thWeekTask/ClassificationMLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import KNNImputer"
      ],
      "metadata": {
        "id": "AUrsgOrnnoV0"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"sample_data/beijing-pm25.csv\")\n",
        ""
      ],
      "metadata": {
        "id": "gcRxKpv_nqq0"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melihat missing values\n",
        "missing_values = data.isnull().sum()\n",
        "print(\"Jumlah missing value di setiap kolom:\")\n",
        "print(missing_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QopsL73nu5z",
        "outputId": "faf733b3-4bc1-4d2e-e0c1-4b445a6479a6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah missing value di setiap kolom:\n",
            "No          0\n",
            "year        0\n",
            "month       0\n",
            "day         0\n",
            "hour        0\n",
            "pm2.5    2067\n",
            "DEWP        0\n",
            "TEMP        0\n",
            "PRES        0\n",
            "cbwd        0\n",
            "Iws         0\n",
            "Is          0\n",
            "Ir          0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengisi missing values pada kolom 'pm2.5' menggunakan KNN Imputer\n",
        "imputer = KNNImputer(n_neighbors=3, weights='uniform')\n",
        "data[['pm2.5']] = imputer.fit_transform(data[['pm2.5']])"
      ],
      "metadata": {
        "id": "uSVYSRqrvp8r"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memastikan bahwa missing value sudah diisi\n",
        "missing_values = data.isnull().sum()\n",
        "print(\"Jumlah missing value di setiap kolom:\")\n",
        "print(missing_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cREZkzt9vsWq",
        "outputId": "4996eb55-59a8-47d7-8b9e-db2dcb540799"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah missing value di setiap kolom:\n",
            "No       0\n",
            "year     0\n",
            "month    0\n",
            "day      0\n",
            "hour     0\n",
            "pm2.5    0\n",
            "DEWP     0\n",
            "TEMP     0\n",
            "PRES     0\n",
            "cbwd     0\n",
            "Iws      0\n",
            "Is       0\n",
            "Ir       0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create target column (classification based on pm2.5 levels)\n",
        "def categorize_pm25(value):\n",
        "    if value <= 35:\n",
        "        return 0  # Low\n",
        "    elif value <= 75:\n",
        "        return 1  # Moderate\n",
        "    else:\n",
        "        return 2  # High\n",
        "\n",
        "data['target'] = data['pm2.5'].apply(categorize_pm25)"
      ],
      "metadata": {
        "id": "qZf_OHlHvxOG"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess data\n",
        "X = data.drop(columns=['No', 'year', 'month', 'day', 'hour', 'pm2.5', 'cbwd', 'target'])\n",
        "# Drop columns that are not features (adjust as needed)\n",
        "y = data['target']"
      ],
      "metadata": {
        "id": "URdHCuVMv9Hr"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "6x9M1jCzxvqx"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "OX-lYdhhxxGV"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)"
      ],
      "metadata": {
        "id": "0VUXqrCexzcD"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
      ],
      "metadata": {
        "id": "mnxdZLhEx1mz"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to define MLP model\n",
        "def create_mlp(input_size, hidden_layers, activation_fn, num_classes):\n",
        "    layers = []\n",
        "    in_features = input_size\n",
        "\n",
        "    # Add hidden layers\n",
        "    for hidden_units in hidden_layers:\n",
        "        layers.append(nn.Linear(in_features, hidden_units))\n",
        "        layers.append(activation_fn())\n",
        "        in_features = hidden_units\n",
        "\n",
        "    # Add output layer\n",
        "    layers.append(nn.Linear(in_features, num_classes))\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "id6EoxMGx3vu"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "input_size = X_train.shape[1]\n",
        "hidden_layer_configs = [[16, 32, 64]]\n",
        "activation_fns = [nn.ReLU, nn.Sigmoid]\n",
        "epochs_list = [100, 250]\n",
        "learning_rates = [1, 0.1,0.01]\n",
        "batch_sizes = [128,256, 512]\n",
        "num_classes = len(np.unique(y))"
      ],
      "metadata": {
        "id": "LFUrUDMSx6o0"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment loop\n",
        "results = []\n",
        "iteration = 1\n",
        "\n",
        "for hidden_layers in hidden_layer_configs:\n",
        "    for activation_fn in activation_fns:\n",
        "        for epochs in epochs_list:\n",
        "            for lr in learning_rates:\n",
        "                for batch_size in batch_sizes:\n",
        "                    print(f\"Iteration {iteration}: Training with hidden_layers={hidden_layers}, activation_function={activation_fn.__name__}, epoch={epochs}, lr={lr}, batch_size={batch_size}\")\n",
        "                    iteration += 1\n",
        "\n",
        "                    # Create model\n",
        "                    model = create_mlp(input_size, hidden_layers, activation_fn, num_classes)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "                    # DataLoader\n",
        "                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "                    # Training loop\n",
        "                    model.train()\n",
        "                    for epoch in range(epochs):\n",
        "                        for X_batch, y_batch in train_loader:\n",
        "                            optimizer.zero_grad()\n",
        "                            outputs = model(X_batch)\n",
        "                            loss = criterion(outputs, y_batch)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                    # Evaluation\n",
        "                    model.eval()\n",
        "                    with torch.no_grad():\n",
        "                        y_pred = model(X_test_tensor)\n",
        "                        y_pred_labels = torch.argmax(y_pred, axis=1)\n",
        "                        acc = accuracy_score(y_test_tensor, y_pred_labels)\n",
        "\n",
        "                    # Save results\n",
        "                    results.append({\n",
        "                        'hidden_layers': hidden_layers,\n",
        "                        'activation_fn': activation_fn.__name__,\n",
        "                        'epochs': epochs,\n",
        "                        'learning_rate': lr,\n",
        "                        'batch_size': batch_size,\n",
        "                        'accuracy': acc\n",
        "                    })\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILJvJZOByCkA",
        "outputId": "de0b5d6b-a63c-4937-8204-76b278712bfc"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Training with hidden_layers=[16, 32, 64], activation_function=ReLU, epoch=100, lr=1, batch_size=128\n",
            "Iteration 2: Training with hidden_layers=[16, 32, 64], activation_function=ReLU, epoch=100, lr=1, batch_size=256\n",
            "Iteration 3: Training with hidden_layers=[16, 32, 64], activation_function=ReLU, epoch=100, lr=1, batch_size=512\n",
            "Iteration 4: Training with hidden_layers=[16, 32, 64], activation_function=ReLU, epoch=100, lr=0.1, batch_size=128\n",
            "Iteration 5: Training with hidden_layers=[16, 32, 64], activation_function=ReLU, epoch=100, lr=0.1, batch_size=256\n",
            "Iteration 6: Training with hidden_layers=[16, 32, 64], activation_function=ReLU, epoch=100, lr=0.1, batch_size=512\n",
            "Iteration 7: Training with hidden_layers=[16, 32, 64], activation_function=ReLU, epoch=100, lr=0.01, batch_size=128\n",
            "Iteration 8: Training with hidden_layers=[16, 32, 64], activation_function=ReLU, epoch=100, lr=0.01, batch_size=256\n",
            "Iteration 9: Training with hidden_layers=[16, 32, 64], activation_function=ReLU, epoch=100, lr=0.01, batch_size=512\n",
            "Iteration 10: Training with hidden_layers=[16, 32, 64], activation_function=ReLU, epoch=250, lr=1, batch_size=128\n",
            "Iteration 11: Training with hidden_layers=[16, 32, 64], activation_function=ReLU, epoch=250, lr=1, batch_size=256\n",
            "Iteration 12: Training with hidden_layers=[16, 32, 64], activation_function=ReLU, epoch=250, lr=1, batch_size=512\n",
            "Iteration 13: Training with hidden_layers=[16, 32, 64], activation_function=ReLU, epoch=250, lr=0.1, batch_size=128\n",
            "Iteration 14: Training with hidden_layers=[16, 32, 64], activation_function=ReLU, epoch=250, lr=0.1, batch_size=256\n",
            "Iteration 15: Training with hidden_layers=[16, 32, 64], activation_function=ReLU, epoch=250, lr=0.1, batch_size=512\n",
            "Iteration 16: Training with hidden_layers=[16, 32, 64], activation_function=ReLU, epoch=250, lr=0.01, batch_size=128\n",
            "Iteration 17: Training with hidden_layers=[16, 32, 64], activation_function=ReLU, epoch=250, lr=0.01, batch_size=256\n",
            "Iteration 18: Training with hidden_layers=[16, 32, 64], activation_function=ReLU, epoch=250, lr=0.01, batch_size=512\n",
            "Iteration 19: Training with hidden_layers=[16, 32, 64], activation_function=Sigmoid, epoch=100, lr=1, batch_size=128\n",
            "Iteration 20: Training with hidden_layers=[16, 32, 64], activation_function=Sigmoid, epoch=100, lr=1, batch_size=256\n",
            "Iteration 21: Training with hidden_layers=[16, 32, 64], activation_function=Sigmoid, epoch=100, lr=1, batch_size=512\n",
            "Iteration 22: Training with hidden_layers=[16, 32, 64], activation_function=Sigmoid, epoch=100, lr=0.1, batch_size=128\n",
            "Iteration 23: Training with hidden_layers=[16, 32, 64], activation_function=Sigmoid, epoch=100, lr=0.1, batch_size=256\n",
            "Iteration 24: Training with hidden_layers=[16, 32, 64], activation_function=Sigmoid, epoch=100, lr=0.1, batch_size=512\n",
            "Iteration 25: Training with hidden_layers=[16, 32, 64], activation_function=Sigmoid, epoch=100, lr=0.01, batch_size=128\n",
            "Iteration 26: Training with hidden_layers=[16, 32, 64], activation_function=Sigmoid, epoch=100, lr=0.01, batch_size=256\n",
            "Iteration 27: Training with hidden_layers=[16, 32, 64], activation_function=Sigmoid, epoch=100, lr=0.01, batch_size=512\n",
            "Iteration 28: Training with hidden_layers=[16, 32, 64], activation_function=Sigmoid, epoch=250, lr=1, batch_size=128\n",
            "Iteration 29: Training with hidden_layers=[16, 32, 64], activation_function=Sigmoid, epoch=250, lr=1, batch_size=256\n",
            "Iteration 30: Training with hidden_layers=[16, 32, 64], activation_function=Sigmoid, epoch=250, lr=1, batch_size=512\n",
            "Iteration 31: Training with hidden_layers=[16, 32, 64], activation_function=Sigmoid, epoch=250, lr=0.1, batch_size=128\n",
            "Iteration 32: Training with hidden_layers=[16, 32, 64], activation_function=Sigmoid, epoch=250, lr=0.1, batch_size=256\n",
            "Iteration 33: Training with hidden_layers=[16, 32, 64], activation_function=Sigmoid, epoch=250, lr=0.1, batch_size=512\n",
            "Iteration 34: Training with hidden_layers=[16, 32, 64], activation_function=Sigmoid, epoch=250, lr=0.01, batch_size=128\n",
            "Iteration 35: Training with hidden_layers=[16, 32, 64], activation_function=Sigmoid, epoch=250, lr=0.01, batch_size=256\n",
            "Iteration 36: Training with hidden_layers=[16, 32, 64], activation_function=Sigmoid, epoch=250, lr=0.01, batch_size=512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan hasil eksperimen\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41Lqar4eEO34",
        "outputId": "b6dc719d-6c78-4dc2-d490-c44c78fb11e7"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   hidden_layers activation_fn  epochs  learning_rate  batch_size  accuracy\n",
            "0   [16, 32, 64]          ReLU     100           1.00         128  0.514204\n",
            "1   [16, 32, 64]          ReLU     100           1.00         256  0.514204\n",
            "2   [16, 32, 64]          ReLU     100           1.00         512  0.514204\n",
            "3   [16, 32, 64]          ReLU     100           0.10         128  0.613805\n",
            "4   [16, 32, 64]          ReLU     100           0.10         256  0.640274\n",
            "5   [16, 32, 64]          ReLU     100           0.10         512  0.645636\n",
            "6   [16, 32, 64]          ReLU     100           0.01         128  0.651683\n",
            "7   [16, 32, 64]          ReLU     100           0.01         256  0.652139\n",
            "8   [16, 32, 64]          ReLU     100           0.01         512  0.651797\n",
            "9   [16, 32, 64]          ReLU     250           1.00         128  0.514204\n",
            "10  [16, 32, 64]          ReLU     250           1.00         256  0.514204\n",
            "11  [16, 32, 64]          ReLU     250           1.00         512  0.514204\n",
            "12  [16, 32, 64]          ReLU     250           0.10         128  0.644153\n",
            "13  [16, 32, 64]          ReLU     250           0.10         256  0.649971\n",
            "14  [16, 32, 64]          ReLU     250           0.10         512  0.640730\n",
            "15  [16, 32, 64]          ReLU     250           0.01         128  0.649173\n",
            "16  [16, 32, 64]          ReLU     250           0.01         256  0.650086\n",
            "17  [16, 32, 64]          ReLU     250           0.01         512  0.653622\n",
            "18  [16, 32, 64]       Sigmoid     100           1.00         128  0.212892\n",
            "19  [16, 32, 64]       Sigmoid     100           1.00         256  0.272904\n",
            "20  [16, 32, 64]       Sigmoid     100           1.00         512  0.514204\n",
            "21  [16, 32, 64]       Sigmoid     100           0.10         128  0.642442\n",
            "22  [16, 32, 64]       Sigmoid     100           0.10         256  0.628523\n",
            "23  [16, 32, 64]       Sigmoid     100           0.10         512  0.649515\n",
            "24  [16, 32, 64]       Sigmoid     100           0.01         128  0.654307\n",
            "25  [16, 32, 64]       Sigmoid     100           0.01         256  0.653052\n",
            "26  [16, 32, 64]       Sigmoid     100           0.01         512  0.653736\n",
            "27  [16, 32, 64]       Sigmoid     250           1.00         128  0.272904\n",
            "28  [16, 32, 64]       Sigmoid     250           1.00         256  0.514204\n",
            "29  [16, 32, 64]       Sigmoid     250           1.00         512  0.272904\n",
            "30  [16, 32, 64]       Sigmoid     250           0.10         128  0.642327\n",
            "31  [16, 32, 64]       Sigmoid     250           0.10         256  0.647461\n",
            "32  [16, 32, 64]       Sigmoid     250           0.10         512  0.648716\n",
            "33  [16, 32, 64]       Sigmoid     250           0.01         128  0.659213\n",
            "34  [16, 32, 64]       Sigmoid     250           0.01         256  0.658985\n",
            "35  [16, 32, 64]       Sigmoid     250           0.01         512  0.656817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the best configuration\n",
        "best_config = max(results, key=lambda x: x['accuracy'])\n",
        "print(\"Best Configuration:\", best_config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArH2s0IGwtPj",
        "outputId": "9df8e3ec-7451-453d-e53e-7a32f37e49cf"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Configuration: {'hidden_layers': [16, 32, 64], 'activation_fn': 'Sigmoid', 'epochs': 250, 'learning_rate': 0.01, 'batch_size': 128, 'accuracy': 0.6592127780946948}\n"
          ]
        }
      ]
    }
  ]
}